# Heritage Housing

Heritage housing is a simple ML system that predicts prices of houses in Ames, Iowa, US, by using a gradient-boosting-based ML regression technique that maps relationships between housing attributes to its sale prices. 

## Table of Content
- [Business Requirements](https://github.com/fokhrun/heritage_housing/blob/documentation/README.md#business-requirements)
- [Dataset](https://github.com/fokhrun/heritage_housing/blob/documentation/README.md#dataset)
- [Hypothesis](https://github.com/fokhrun/heritage_housing/blob/documentation/README.md#dataset)
- [Mapping Business Case To ML Solution](https://github.com/fokhrun/heritage_housing/blob/documentation/README.md#mapping-business-case-to-ml-solution)
- [Software Development](https://github.com/fokhrun/heritage_housing//blob/documentation/README.md#software-development)
- [Planning & Execution](https://github.com/fokhrun/heritage_housing/blob/documentation/README.md#planning--execution)
- [Future Improvements](https://github.com/fokhrun/heritage_housing/blob/documentation/README.md#future-improvements)
- [Credits](https://github.com/fokhrun/heritage_housing/blob/documentation/README.md#credits)

## Business requirements
The client is interested in a dashboard application that allows her to maximize the sale price of her inherited houses in the Ames, Iowa area, as well as any other houses in that area. The dashboard should include 
- prediction of house sale prices from her 4 inherited houses, and any other house in Ames, Iowa. The prediction should be generated by a reliable ML system.  
- data visualizations of training data that show correlations of the properties of houses against the sale price. It is sufficient to use conventional data analysis techniques.
- A way to explain the predictions made by the estimator. The estimator should demonstrate an R2 score of at least 0.75 on the training and the test data.
- Hypothesis used in the project and how it has been validated

## Dataset
The dataset used in this project is a curated version of the publicly available Ames Housing dataset. The dataset contains 24 explanatory variables describing key attributes of residential homes and their sale prices in Ames, Iowa. It contains unique records of 1430 houses. The dataset can be downloaded from [here](https://www.kaggle.com/datasets/codeinstitute/housing-prices-data).

|  Num | Variables | Type | Description | Value |
|---|---|---|---|---|
| 1 | 1stFlrSF | numerical | First Floor square feet | 334 - 4692 |
| 2 | 2ndFlrSF | numerical | Second floor square feet |	0 - 2065 |
| 3	| BedroomAbvGr | categorical | Bedrooms above grade (does NOT include basement | 0 - 8 |)
| 4	| BsmtExposure | categorical | Refers to walkout or garden level walls | Gd: Good Exposure, Av: Average Exposure, ...|
| 5	| BsmtFinType1 | categorical | Rating of basement finished area | GLQ: Good Living Quarters, ALQ: Average Living, ...|
| 6	| BsmtFinSF1 | numerical | Type 1 finished square feet | 0 - 5644 |
| 7	| BsmtUnfSF	| numerical	| Unfinished square feet of basement area |	0 - 2336 |
| 8	| TotalBsmtSF |	numerical |	Total square feet of basement area | 0 - 6110 |
| 9 | GarageArea | numerical | Size of garage in square feet | 0 - 1418 |
| 10 | GarageFinish | categorical | Interior finish of the garage |	Fin: Finished, RFn: Rough Finished, Unf: Unfinished, ...|
| 11 | GarageYrBlt | temporal |	Year garage was built |	1900 - 2010 |
| 12 | GrLivArea | numerical | Above grade (ground) living area square feet | 334 - 5642 |
| 13 | KitchenQual | categorical | Kitchen quality | Ex: Excellent, Gd: Good, TA: Typical/Average, ... |
| 14 | LotArea | numerical | Lot size in square feet |	1300 - 215245 |
| 15 | LotFrontage | numerical | Linear feet of street connected to property | 21 - 313 |
| 16 | MasVnrArea |	numerical | Masonry veneer area in square feet | 0 - 1600 |
| 17 | EnclosedPorch | numerical |	Enclosed porch area in square feet | 0 - 286 |
| 18 | OpenPorchSF | numerical | Open porch area in square feet | 0 - 547 |
| 19 | OverallCond | categorical |	Rates the overall condition of the house |	10: Very Excellent, 9: Excellent, 8: Very Good, ...|
| 20 | OverallQual | categorical |	Rates the overall material and finish of the house | 10: Very Excellent, 9: Excellent, 8: Very Good...|
| 21 | WoodDeckSF |	numerical |	Wood deck area in square feet |	0 - 736 |
| 22 | YearBuilt |	temporal |	Original construction date | 1872 - 2010 |
| 23 | YearRemodAdd | temporal | Remodel date |	1950 - 2010 |
| 24 | SalePrice | numerical | Sale Price |	34900 - 755000 |

## Hypothesis

The target variable in this project is SalePrice, which represents the sale price of houses. The factors that affect these prices are:

- size:
    - hypothesis: The larger the property, the higher the price
    - relevant variables: 1stFlrSF, 2ndFlrSF, BsmtFinSF1, BsmtUnfSF, TotalBsmtSF, GarageArea, GrLivArea, LotArea, LotFrontage, MasVnrArea, EnclosedPorch, WoodDeckSF
- condition:
    - hypothesis: The better the condition, the higher the price
    - relevant variables: BedroomAbvGr, BsmtExposure, KitchenQual, BsmtFinType1, GarageFinish, OverallCond, OverallQual
- age: The newer the house, the higher the price
    - hypothesis: The newer the house, the higher the price
    - relevant variables: GarageYrBlt, YearBuilt, YearRemodAdd

### Validation Approach
These hypotheses should be validated by the following observations:
- Both the actual and predicted (on data unused in training) SalePrice should be very strongly correlated
- The predicted (on data unused in training) SalePrice should generally increase with that of the house size, condition, and age. It should show a correlation to the columns mentioned above similarly to the actual sale price.

Note that location desirability and room count also have similar effects, but the dataset did not have those variables.

## Mapping Business Case To ML Solution

### How the data is collected and cleansed

Before we handle any of the business cases, we need to acquire the data. Even though the data can be acquired using a plain Python script, we have implemented it as a jupyter notebook titled [Data Collection](https://github.com/fokhrun/heritage_housing/blob/documentation/jupyter_notebooks/data_collection.ipynb). The notebook performs the following:

1. Download the data from kaggle. It is a zip file that is extracted. It provides three main data: 
    1. `house-metadata.txt`: contains a description of the attributes 
    2. `house_prices_records.csv`: contains data to be used for ML model training
    3. `inherited_houses.csv`: contains data to be used for ML model prediction

2. Read the `house_prices_records.csv` and `inherited_houses.csv` files as pandas dataframes, validate the shapes, and get a basic understanding of the dataset. Apart from the number of rows (1460 for the former and 4 for the latter), the difference between these two dataframes is the `SalePrice` attribute, which is only available in `house_prices_records.csv`. This is the target attribute. 

3. Analyze the missing data. `inherited_houses.csv` does not have any. However, the `house_prices_records.csv` has some. Observe the following image for more details. 

![Missing Values](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/missing_data.png)

### How the House attributes correlate with the sale price

To study the housing attributes and its relationship with the target variable `SalePrice`, we implemented a Jupyter notebook titled [Exploratory Data Analysis](https://github.com/fokhrun/heritage_housing/blob/documentation/jupyter_notebooks/exploratory_data_analysis.ipynb). The notebook performs the following steps:

1. Read `house-metadata.txt` and prepares housing attribute descriptions as a table with the following columns `featureName`, `featureType` (inferred categorisation of features as numerical, categorical, or temporal), `featureDescription`, and `featureValues`.
2. Read `house_prices_records.csv`, analyze its missing data, and remove columns that have 10% or more data missing. 
3. Analyse numerical, temporal, and categorical columns as well as target variables. 
4. Analyse the correlation of the housing attributes to the target variable. 
5. Identify variables with strong and moderate correlations and preserve them to be used in the next steps. 

### Analyse the correlation to the target variable

#### Numerical and temporal attributes

We used a scatter plot between each numerical and temporal variable with a target variable. To measure the correlation we used [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) as a score titled `corr`. We used the score to also identify each column as having `very weak correlation` (`corr` < 0.3), `weak correlation` (0.3 =< `corr` < 0.5),  `moderate correlation` (0.5 =< `corr` < 0.7), and `strong correlation` (`corr` >= 0.7). 

The analysis is demonstrated in the following figures. 

![Correlation to numerical variables (group 1)](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/correlation_numerical_1.png)
![Correlation to numerical variables (group 2)](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/correlation_numerical_2.png)

From these variables, the following have a `corr` score of 0.5 or higher. 

|  Variable | Score | Correlation Category |
|---|---|---|
| GrLivArea | 0.71 | strong |
| GarageArea | 0.62 | moderate |
| TotalBsmtSF | 0.61 | moderate |
| 1stFlrSF | 0.61 | moderate |
| YearBuilt | 0.52 | moderate |
| YearRemodAdd | 0.51 | moderate |

#### Categorical attributes

For categorical attributes, I used box plots of the target variable per category for each attribute. The categorical variables have orders as they fit Likert-like scales. To evaluate the correlations, we visually inspected if category order indicates reasonable sale price improvement. Please see the following image for more details. The visual inspection indicated that the following variables have a moderate to strong correlation: `KitchenQual`, `OverallCond`, and `OverallQual`.

![Correlation to categorical variables](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/correlation_categorical.png)

### How the trained model generates good predictions

The model training process is implemented using the Jupyter notebook titled [Model Training, Optimization, and Validation](https://github.com/fokhrun/heritage_housing/blob/documentation/jupyter_notebooks/model_training.ipynb)

The process consists of the following steps:
1. Data loading
2. Feature Engineering
3. Hyperparameter Tuning
4. Model Training
5. Model Performance Validation 

#### Feature Engineering

Our approach to feature engineering is as follows:
1. Only use correlated attributes from the outcomes of exploratory data analysis: `GrLivArea`, `GarageArea`, `TotalBsmtSF`, `1stFlrSF`, `YearBuilt`, `YearRemodAdd`, `KitchenQual`, `OverallCond`, and `OverallQual`.
2. Instead of feeding attributes directly, we will create bins out of the target variable, group and aggregate attributes on the bin values (a concept inspired by histogram), and expand aggregated features with sale prices matching the bins. 

This way of feature engineering achieves the following:
1. Efficient model training process due to a small number of variables (9 instead of 23), hopefully without sacrificing the performance (due to the strong correlation nature). 
2. A feature engineering process that would allow solving this ML problem both as a regression and classification problem. The `SalePrice` bins can also be treated as house price classes. 

##### Binning SalePrice

We followed a simple approach. We created a histogram out of the `SalePrice`, picked the highest value from each histogram bin as the chosen `SalePrice` bin values, and mapped the `SalePrice` to these chosen values. There were 39 bins. 

##### Categorical features

There were three categorical attributes. We followed the approach below:
1. For each categorical variable, we created a dummy variable for each of its categories. There were three categorical variables with 10, 10, and 5 categories, which created 25 dummy variables. Some category values were missing. So, we ultimately had 23 dummy variables. 
2. Then for each 23 dummy variables, we grouped them based on `SalePrice` bin values and aggregated the group using `sum` and `mean` functions. This process ultimately gave us 46 features.  

##### Numerical features

There were four chosen numerical attributes. For each of them, we grouped them based on `SalePrice` bin values and aggregated the group using `count`, `mean`, `max`, `min`, and `sum` functions. This process ultimately gave us 20 features taking the total count of features to 66.

##### Temporal features

There were two chosen temporal variables, each representing years. we picked the highest year in one of the columns and used it as the most recent year. Then we calculated the number of years using each value in the temporal columns from the most recent date using that value. These gave us 2 features, taking the total count of features to 68.

##### Combining features

After combining features from all three threads, we got 69 variables, 68 from the features and 1 from the `SalePrice` bin. Then we joined that dataset with the `SalePrice` and `SalePrice` bins. That gave us a 70 x 1460 matrix. 

##### Splitting the features for training and testing

We decided to split the feature matrix for training and testing purposes. The training dataset has 80% of the dataset. Although, during hyperparameter tuning, we sampled 50% of the training dataset again. We will use the remaining 20% for testing the model on known but unseen data.

#### Hyperparameter tuning

We decided to use the Random Search technique over an [XGBoost estimator](https://xgboost.readthedocs.io/en/stable/) for the following parameter configurations:

- `n_estimators`: 500, 550, 600, 650, ..., 2000 
- `learning_rate`: 0.05, 0.06, 0.07 
- `max_depth`: 3, 5, 7
- `min_child_weight`: 1, 1.5, 2

The parameter optimization ran for about 1.12 minutes. It generated an r2 score of 0.99 and 0.97 on the full training feature and testing feature respectively with its best parameter, which happens to be `n_estimators`: 550, `min_child_weight`: 1.5, `max_depth`: 5, and `learning_rate`: 0.06. 

The estimator with the best parameter had the following feature importance score for the top 10 contributing features:

![Feature Importance Optimisation](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/feature_importance_opt.png)

#### Model training

The model is trained using XGBoost using the best parameters of the hyperparameter tuning over the full training feature matrix. It only took a few seconds and generated an r2 score of 1 and 0.99 on the full training feature and testing feature matrix respectively. 

The estimator had the following feature importance score for the top 10 contributing features:

![Feature Importance Optimisation](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/feature_importance_ml.png)

#### Saving and using the model

We saved the model in the `joblib` serialized format. The serialized model is loaded in memory again and tested against the `inherited_houses` dataset that generated good-looking predictions. 

#### Evaluating if predicted and actual values have similar correlations to housing attributes

We wanted to validate that the size (numerical) attributes would yield higher `SalePrice` for its larger values and lower for its lower values. Similarly, validation needs to happen on condition and age attributes. 

To validate the hypothesis, we compared the correlation of housing attributes to both actual and predicted `SalePrice` based on the testing dataset. The following table demonstrated that the actual and predicted (on data unused in training) `SalePrice` are quite strongly correlated. 

![Correlation Predicted Actuals](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/correlation_predicted_actuals.png)

The following scatterplots demonstrate that the predicted (on data unused in training) `SalePrice` generally increases with the that of the house size, condition, and age. It shows a correlation to the columns mentioned above similarly to the actual sale price.

![Correlation Predicted Actuals Scatter](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/correlation_predicted_actuals_scatter.png)

### How key results are demonstrated in the ML dashboard application

The objectives of the ML dashboard are given as follows:

1. Provide an overview of the overall business problem
2. Demonstrate how to leverage the model to predict house prices
3. Demonstrate how to understand the training data and its correlation to the target variable
4. Demonstrate how to understand the performance of the model

The ML dashboard is developed as a multipage Streamlit app that realizes the above objectives. The live version of the dashboard can be found [here](https://binita-heritage-housing-cb7da8771259.herokuapp.com/). 

![Dashboard page](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/dashboard_pages.png)

The first page presents the same information as covered in [Business Requirements](https://github.com/fokhrun/heritage_housing/blob/documentation/README.md#business-requirements). The other pages are covered in the following sections.

#### House price prediction dashboard page

This is the most interactive part of the application that provides prediction results to inherited houses, but also provides prediction to any combination of housing attributes. 

The page has two subsections:

1. Providing input for housing attributes that will be used for prediction
2. Showing results

The first subsection includes a set of radio buttons, sliders, and buttons that allow a user to create a prediction result. Usually for house condition attributes radio buttons are used and for both housing size and age attributes sliders are used. To submit a prediction call or resetting buttons are used. Note that all attributes are included. The model is trained using the selected set of attributes, which are also used to generate the predictions. The following image shows the input section.  

![prediction input](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/prediction_input.png)

The second subsection includes a table where prediction results are shown. This section already shows the `SalePrice` of the four inherited houses. However, a fifth `SalePrice` is shown which is the predicted price of custom inputs provided in the earlier subsection. 

The predictions are generated by the ML model developed in this project, which is loaded in memory with the dashboard. The predictions are marked as blue to highlight more prominently. The results also show selected attributes connected to the `SalePrice` to provide context to the predictions. The following images show the prediction section before the custom prediction job and after the custom prediction job. 

![prediction results](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/prediction_results.png)

![prediction results with input](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/prediction_result_input_based.png)

#### Training data analysis page

This page contains two subsections separated by tabs:

1. Basic training data analysis
2. Correlation of housing attributes to the `SalePrice` attribute

The first subsection allows observing a view of the training data as well as the distribution of the attributes, data description statistics, and missing values. While the data view is always visible, the other options are only visible through the expander, which a user can choose to see or not. Since there are 24 attributes, it is not easy to visualize. Therefore we only show 6 attributes at a time, which is selected using a number input. Check the following screenshot to get a brief understanding of the subsection:

![training data overview](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/training_data_overview.png)

The second subsection shows the correlation of housing attributes to the `SalePrice` variable. 
- It always shows the information of highly correlated features
- It shows a scatterplot of highly correlated and low correlated features to `SalePrice` through expanders that a user can use to see or not. 

Check the following screenshot to get a brief understanding of the subsection:

![correlation to sale price](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/correlation_to_saleprice_page.png)

#### Training performance analysis page

This page contains three subsections separated by tabs:

1. Project hypothesis
2. Performance of ml model training that uses the best parameters from the hyperparameter tuning
3. Performance of hyperparameter tuning that tries to search for the best parameters

The first subsection shows the same information as in - [Hypothesis](https://github.com/fokhrun/heritage_housing/blob/documentation/README.md#dataset).

The second subsection shows the performance of the ML model training. First of all, it shows what r2 and mse scores the model has on training features (seen data) and testing features (unseen data). It also shows what are key parameters leveraged by the estimator used for prediction. In addition, it also shows how the estimator learned through an expander, which a user can choose to see. Finally, it shows how hypotheses in the projects are validated by the estimator through a set of scatterplots as well as key conclusions. The following image shows the screenshot of the subsection:

![Training performance](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/modelling_performance_page.png)

The third subsection shows the hyperparameter tuning performance. First, it shows the performance of the best estimator searched by the tuning process on both training and testing data on r2 and mse scores. It also shows the performance of the top tried estimators, highlighting the best estimator. Both of these pieces of information are on the expander, which allows users to choose to see or not.

![Training performance](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/hyperparameter_tuning_page.png)

To learn more about the training and hyperparameter tuning process and performance, check the section [How the trained model generates good predictions](https://github.com/fokhrun/heritage_housing/blob/documentation/README.md#how-the-trained-model-generates-good-predictions)

## Software Development

### Tech Stack

The main tech stack for this project is as follows:

- Programming Language: `Python` (3.11.6)
- Machine Learning and data analysis
    - `jupyter`: for interacting and visual-heavy development
    - `pandas` (2.1.3) & `numpy` (1.26.2): large dataset wrangling
    - `xgboost` (2.0.3) & `scikit-learn` (1.3.2): ML and statistical work
    - `matplotlib` (3.8.2) & `seaborn` (0.13.0): data visualization
    - `kaggle` (1.5.16): for data sourcing
- Dashboard/Application: `streamlit` (1.29.0)
- Environment management: 
    - `pip` (23.3.1): Python library management
    - `venv` 
    - `python-dotenv` (1.0.0): Handle environment variables
- Version control: `GitHub`
- Project Management: `GitHub projects` 
- Production Environment: `Heroku`
- IDE: `VS Code` (1.85.1)

### Development Environment

The initial GitHub repo starts from the starter repo [milestone-project-heritage-housing-issues](Code-Institute-Solutions/milestone-project-heritage-housing-issues) provided by Code Institute. The starter repo includes a notebook folder structure, configuration examples of deploying the streamlit dashboard to Heroku, and a starter `requirements.txt` file. 

The project is cloned in a local VS code workspace. It leverages pip and venv modules to install and isolate the execution environment. Notice that Python and other Python library versions are more modern in this project than what is provided by the starter repo. This came through repeated trial/error of what library version would work that would not come in the way of deploying the streamlit dashboard to Heroku. 

To start developing with this repo, use the following procedure:
```
1. Ensure that you have a Python version of at least Python v3.11 and pip v23.3.1
2. Clone the repository https://github.com/fokhrun/heritage_housing.git as heritage_housing in your local directory
3. Work inside heritage_housing
```

We strongly recommend using venv as follows:
```
1. python3 -m venv venv
2. .\venv\Scripts\activate
3. pip install -r .\requirements.txt
```

#### Working with notebooks for ML work
To start working with jupyter notebooks, run the command `jupyter notebook`. It will launch a jupyter workspace. Start working with the existing notebooks. Currently, there are three notebooks:
1. `data_collection.ipynb`: collecting data from kaggle.com
2. `exploratory_data_analysis.ipynb`: training data analysis including correlation study
3. `model_training.ipynb`: ML model training to be used for house price prediction

One can create a new one from the `Notebook_Template.ipynb`, the template came from the starter repo and is quite useful. 
However, we chose to handle environment variables differently than the template notebook recommends. Basically, not only, do we define all secret information as environment variables, but we also use define all path and filename variables as environment variables. There are other ways to do it, but it is quite effective to use the same variable everywhere. 

#### Working with streamlit dashboard

To start working with streamlit, run the command `streamlit run Heritage_Housing.py`. Then it will start a streamlit application in the browser. 

The top page of the dashboard is always defined by the main app page, `Heritage_Housing.py`, in our case. This file contains project overview information. All the other pages are created under the pages folder. We have three such page python codes:

1. `1_Housing_Price_Prediction.py`: prediction generation
2. `2_Housing_Data.py`: Training data analysis overview
3. `3_Training_Performance.py`: Hypothesis and performance of the training process

Note that, the naming style influences how the dashboard page menu is created. For example, the number influences how the pages are ordered and the case and spacing of the files influence how the title of each page is shown in the menu. 

In the case of the streamlit dashboard, almost all data and file resources are cached. If you are changing the data or files, make sure to restart the service. 

### Testing

No automated unit tests were implemented for the Python files, due to lack of time. However, each Python file has been cleansed to remove pep8 issues. We did it using the warnings raised by the command

    `pylint pylint .\utils\ .\pages\ .\Heritage_Housing.py`

It generated a score of 10 out of 10. A few warnings have been disabled to take care of unavoidable issues. 

### Testing notebook files

Only data shapes are tested using the assert function, such as follows:

```assert inherited_houses_numerical.shape == (prediction_subset.shape[0], len(numerical_feature_names))```

The best way to run these tests is to execute the notebooks from top to bottom.

### Testing streamlit app

A few data shapes are tested using the assert function similar to the above. 
The dashboard has been tested manually as follows:

- `1_Housing_Price_Prediction.py` is tested by choosing the same inputs as one of the inherited houses. If it generates the same `SalePrice` it is generating a consistent result. It is also tested if clicking the `Reset` button resets the prediction results to four inherited houses, once a prediction has been generated. 
- `2_Housing_Data.py` and `3_Training_Performance.py` are tested by clicking each tab and expander and observing if the information seems visually correct. They are further verified by comparing them against similar outputs in `exploratory_data_analysis.ipynb` and `model_training.ipynb` respectively.

### Deployment to production

The app is deployed in Heroku.

To prepare for the deployment, we used three files: 

- `runtime.txt`: should have `python-3.11.6`
- `setup.sh`: keep it as it came with the starter template. Add the line `mv .env_template_unix .env` so that the correct env variables are loaded in the Heroku machine. 
- `Procfile`: should have web: `sh setup.sh && streamlit run Heritage_Housing.py`

To deploy use the following instructions:

1. Create a new app in [Heroku](https://dashboard.heroku.com/apps)
2. Fill the form with the app name of your choosing, e.g., "heritage_housing" and "Europe" as the region
3. Stay with the Heroku-22 stack
4. Select Deploy
    1. Choose "GitHub
    2. Search the GitHub repo name `heritage_housing`. Once the repo is found, connect the repo.
    3. Choose the default branch to deploy, i.e., the `main` branch. If you are working with another branch, choose accordingly.
    4. Click "Enable Automatic Deploys" which allows the app to be redeployed for every commit.
    5. Click "Deploy Branch" for the first manual push.
5. Select Open app to verify that the app has been deployed.

### Fixed bugs

1. While choosing highly correlated variables, only moderate correlations were used due to an input mistake. It led to a lower r2 score, as variables with high correlation were ignored. It was fixed afterward. 
2. While running hyperparameter tuning, mse with positive scoring was chosen, where it should have been a negative score. That also led to a lower r2 score. It was fixed afterward.

### Unfixed bugs

Some element of the dashboards loads a bit slowly the first time. This is because some of the plots are calculated on demand. These plots should be pre-computed and loaded as a file in the dashboard.

## Planning & Execution

The project followed a simple agile method, with the 5 main epics:
- Bootstrap
- Data collection
- Data analysis
- Model training
- Dashboard
- Documentation

The epics were executed with one or more user stories in the GitHub Project [Heritage Housing](https://github.com/users/fokhrun/projects/3/views/1?filterQuery=). The following image provides a snapshot of the project:

![Tracking user stories in GitHub Project](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/github_project_heritage_housing.png)

The project is designed as a kanban board with three columns: `To Do`, `In Progress`, and `Done`.

Each user story is created by adding an item in the `To Do` column. When work starts for a user story, it is moved to `In Progress`. When the work regarding the user story finishes, it is moved to the `Done` column. The following figure demonstrates a user story. 

![User story](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/user_story.png)

### Execution

Each user story is linked to a GitHub issue as demonstrated by the following figure. It allows connecting the user story to the relevant GitHub branch/pull request. 

![Issue connected to user story](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/issues_connected_to_user_stories.png)

The execution of the user story typically starts at the codebase, with a branch off the main branch. Once the development is done, the branch is merged to the main branch using a pull request (see below figure).

![Pull request](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/pull_request.png)

### User Stories

|  Epic | User Stories |
|---|---|
| Bootstrap | [Starter repository](https://github.com/users/fokhrun/projects/3/views/1?filterQuery=&pane=issue&itemId=46559138) |
|  | [Local development environment](https://github.com/users/fokhrun/projects/3/views/1?filterQuery=epic%3ABootstrap&pane=issue&itemId=46693177) |
| Data collection | [Prepare for data collection](https://github.com/users/fokhrun/projects/3/views/1?filterQuery=epic%3A%22Data+collection%22&pane=issue&itemId=46690864)
|  | [Data collection notebook](https://github.com/users/fokhrun/projects/3/views/1?filterQuery=epic%3A%22Data+collection%22&pane=issue&itemId=46560003) |
| Data Analysis | [Exploratory data analysis](https://github.com/users/fokhrun/projects/3/views/1?filterQuery=epic%3A%22Data+Analysis%22&pane=issue&itemId=46694465)
| Model Training | [Model training, optimization and validation](https://github.com/users/fokhrun/projects/3/views/1?filterQuery=epic%3A%22Model+Training%22&pane=issue&itemId=46690073) |
| Dashboard | [Dashboard planning, designing, and development](https://github.com/users/fokhrun/projects/3/views/1?filterQuery=&pane=issue&itemId=46690128) |
|  | [Dashboard deployment and release](https://github.com/fokhrun/heritage_housing/issues/12) |
| Documentation | [Documentation](https://github.com/users/fokhrun/projects/3/views/1?filterQuery=&pane=issue&itemId=49358304) |

## Future Improvements

1. The bulk of the code used in the Jupyter Notebooks is written as flat Python script. A sizable part of the code should be placed inside functions in order to reduce similar-looking codes. It would also allow testing part of jupyter notebook code in a better way.
2. Implement automated unit testing for Python functions. 
3. More variables can be used in feature engineering to see if they affect modeling performance without consuming too much time. 
4. Feature engineering with plain variables should be tried to understand if it affects modeling performance.
5. Other hyperparameter tuning approaches, such as Grid Search, HyperOpt, etc., should be tried to understand if it affect modeling performance and time. 
6. Other estimators, such as Random Forest, LightGBM, etc., should be tried to compare modeling performance.
7. A classification-based technique can tried to compare modeling performance. 

## Credits 

1. Notebook codes: While no code written by others has been used directly, we explore several notebooks to draw inspiration and get a better understanding. The notebooks can be found in [House Prices - Advanced Regression Techniques](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/code).
2. Streamlit codes: In addition to the official streamlit documentation, this [article](https://medium.com/streamlit/paginating-dataframes-with-streamlit-2da29b080920) is used to implement the pagination technique in one of the pages in the dashboard. 
3. Overall many [stackoverflow](https://stackoverflow.com/) posts have been studied to figure out how to manipulate pandas dataframe effectively. No direct code has been used though.

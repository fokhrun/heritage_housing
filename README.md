# Heritage Housing

Heritage housing is an simple ML system that predicts prices of houses in Ames, Iowa, US, by using gradient-boosting based ML regression technique that maps relationships between housing attributes to its sale prices. 

## Table of Content
- [Business Requirements](https://github.com/fokhrun/heritage_housing/blob/documentation/README.md#business-requirements)
- [Dataset](https://github.com/fokhrun/heritage_housing/blob/documentation/README.md#dataset)
- [Hypothesis](https://github.com/fokhrun/heritage_housing/blob/documentation/README.md#dataset)
- [Mapping Business Case To ML Solution](https://github.com/fokhrun/heritage_housing/blob/documentation/README.md#mapping-business-case-to-ml-solution)
- [Planning & Execution](https://github.com/fokhrun/heritage_housing/blob/documentation/README.md#planning--execution)


## Business requirements
The client is interested in a dashboard application that allows her to maximize sale price of her inherited houses in the Ames, Iowa area, as well as any other houses in that area. The dashboard should includes 
- prediction of house sale prices from her 4 inherited houses, and any other house in Ames, Iowa. The prediction should be generated by a reliable ML system.  
- data visualizations of training data that shows correlations of the properties of houses against the sale price. It is sufficient to use conventional data analysis techniques.
- A way to explain the predictions made by the estimator. The estimator should demonstrate an R2 score of at least 0.75 on the training and the test data.
- Hypothesis used in the project and how it has been validated

## Dataset
The dataset used in this project is a curated version of publicly available Ames Housing dataset. The dataset contains 24 explanatory variables describing key attributes of residential homes and their sale prices in Ames, Iowa. It contains unique records of 1430 houses. The dataset can be downloaded from [here](https://www.kaggle.com/datasets/codeinstitute/housing-prices-data).

|  Num | Variables | Type | Description | Value |
|---|---|---|---|---|
| 1 | 1stFlrSF | numerical | First Floor square feet | 334 - 4692 |
| 2 | 2ndFlrSF | numerical | Second floor square feet |	0 - 2065 |
| 3	| BedroomAbvGr | categorical | Bedrooms above grade (does NOT include baseme... | 0 - 8 |
| 4	| BsmtExposure | categorical | Refers to walkout or garden level walls | Gd: Good Exposure, Av: Average Exposure, ...|
| 5	| BsmtFinType1 | categorical | Rating of basement finished area | GLQ: Good Living Quarters, ALQ: Average Living, ...|
| 6	| BsmtFinSF1 | numerical | Type 1 finished square feet | 0 - 5644 |
| 7	| BsmtUnfSF	| numerical	| Unfinished square feet of basement area |	0 - 2336 |
| 8	| TotalBsmtSF |	numerical |	Total square feet of basement area | 0 - 6110 |
| 9 | GarageArea | numerical | Size of garage in square feet | 0 - 1418 |
| 10 | GarageFinish | categorical | Interior finish of the garage |	Fin: Finished, RFn: Rough Finished, Unf: Unfinished, ...|
| 11 | GarageYrBlt | temporal |	Year garage was built |	1900 - 2010 |
| 12 | GrLivArea | numerical | Above grade (ground) living area square feet | 334 - 5642 |
| 13 | KitchenQual | categorical | Kitchen quality | Ex: Excellent, Gd: Good, TA: Typical/Average, ... |
| 14 | LotArea | numerical | Lot size in square feet |	1300 - 215245 |
| 15 | LotFrontage | numerical | Linear feet of street connected to property | 21 - 313 |
| 16 | MasVnrArea |	numerical | Masonry veneer area in square feet | 0 - 1600 |
| 17 | EnclosedPorch | numerical |	Enclosed porch area in square feet | 0 - 286 |
| 18 | OpenPorchSF | numerical | Open porch area in square feet | 0 - 547 |
| 19 | OverallCond | categorical |	Rates the overall condition of the house |	10: Very Excellent, 9: Excellent, 8: Very Good, ...|
| 20 | OverallQual | categorical |	Rates the overall material and finish of the house | 10: Very Excellent, 9: Excellent, 8: Very Good...|
| 21 | WoodDeckSF |	numerical |	Wood deck area in square feet |	0 - 736 |
| 22 | YearBuilt |	temporal |	Original construction date | 1872 - 2010 |
| 23 | YearRemodAdd | temporal | Remodel date |	1950 - 2010 |
| 24 | SalePrice | numerical | Sale Price |	34900 - 755000 |

## Hypothesis

The target variable in this project is SalePrice, which represent the sale price of houses. The factors that effect these prices are:

- size:
    - hypothesis: larger the property, higher should be the price
    - relevant variables: 1stFlrSF, 2ndFlrSF, BsmtFinSF1, BsmtUnfSF, TotalBsmtSF, GarageArea, GrLivArea, LotArea, LotFrontage, MasVnrArea, EnclosedPorch, WoodDeckSF
- condition:
    - hypothesis: better the condition, higher should be the price
    - relevant variables: BedroomAbvGr, BsmtExposure, KitchenQual, BsmtFinType1, GarageFinish, OverallCond, OverallQual
- age: newer the house, higher should be the price
    - hypothesis: newer the house, higher the price
    - relevant variables: GarageYrBlt,YearBuilt, YearRemodAdd

### Validation Approach
These hypothesis should be validated by the following observations:
- Both the actual and predicted (on data unused in training) SalePrice should be very strongly correlated
- The predicted (on data unused in training) SalePrice should generally increase with the that of the house size, condition, and age. It should show correlation to the columns mentioned above similarly to the actual sale price.

Note that location desirability and room count also have similar effect, but the dataset did not those variables.

## Mapping Business Case To ML Solution

### How the data is collected and cleansed

Before we handle any of the business case, we need to acquire the data. Even though the data can be acquired using a plain Python script, we have implemented it as a jupyter notebook titled [Data Collection](https://github.com/fokhrun/heritage_housing/blob/documentation/jupyter_notebooks/data_collection.ipynb). The notebook performs the following:

1. Download the data from kaggle. It is a zip file that is extracted. It provides three main data: 
    1. `house-metadata.txt`: contains description of the attributes 
    2. `house_prices_records.csv`: contains data to be used for ML model training
    3. `inherited_houses.csv`: contains data to be used for ML model prediction

2. Read the `house_prices_records.csv` and `inherited_houses.csv` files as pandas dataframes, validate the shapes, and get a basic understanding of the dataset. Apart from the number of rows (1460 for the former and 4 for the latter), the difference between these two dataframes is the `SalePrice` attribute, which is only available in `house_prices_records.csv`. This is the target attribute. 

3. Analyze the missing data. `inherited_houses.csv` does not have any. However, the `house_prices_records.csv` has some. Observe the following image for more details. 

![Missing Values](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/missing_data.png)

### How the House attributes correlate with the sale price

To study the housing attributes and its relationship with the target variable `SalePrice`, we implemented a Jupyter notebook titled [Exploratory Data Analysis](https://github.com/fokhrun/heritage_housing/blob/documentation/jupyter_notebooks/exploratory_data_analysis.ipynb). The notebook performs the following steps:

1. Read `house-metadata.txt` and prepares housing attribute descriptions as a table with the following columns `featureName`, `featureType` (inferred categorisation of features as numerical, categorical, or temporal), `featureDescription`, and `featureValues`.
2. Read `house_prices_records.csv`, analyse its missing data, and remove columns that have 10% or more data missing. 
3. Analyse numerical, temporal, and categorical columns as well target variable. 
4. Analyse correlation of the housing attributes to the target variable. 
5. Identify variables with strong and moderate correlations and preserve them to be used in the next steps. 

### Analyse correlation to target variable

#### Numerical and temporal attributes

We used scatter plot between each numerical and temporal variables with target variable. To measure the correlation we used [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) as score titled `corr`. We used the score to also identify each column to have `very weak correlation` (`corr` < 0.3), `weak correlation` (0.3 =< `corr` < 0.5),  `moderate correlation` (0.5 =< `corr` < 0.7), and `strong correlation` (`corr` >= 0.7). 

The analysis is demonstrated in the following figures. 

![Correlation to numerical variables (group 1)](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/correlation_numerical_1.png)
![Correlation to numerical variables (group 2)](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/correlation_numerical_2.png)

From these variables, the following have a `corr` score 0.5 or higher. 

|  Variable | Score | Correlation Category |
|---|---|---|
| GrLivArea | 0.71 | strong |
| GarageArea | 0.62 | moderate |
| TotalBsmtSF | 0.61 | moderate |
| 1stFlrSF | 0.61 | moderate |
| YearBuilt | 0.52 | moderate |
| YearRemodAdd | 0.51 | moderate |

#### Categorical attributes

For categorical attributes, I used box plots of target variable per category for each attributes. The categorical variables are have orders as they fit likert-like scales. To evaluate the correlations, we visually inspected if category order indicates reasonable sale price improvement. Please see the following image for more details. The visual inspection indicated that the following variables have a moderate to strong correlation: `KitchenQual`, `OverallCond`, `OverallQual`.

![Correlation to categorical variables](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/correlation_categorical.png)

### How the trained model generates good predictions

The model training is process is implemented using the Jupyter notebook titled [Model Training, Optimization, and Validation](https://github.com/fokhrun/heritage_housing/blob/documentation/jupyter_notebooks/model_training.ipynb)

The process consists of the following steps:
1. Data loading
2. Feature Engineering
3. Hyperparameter Tuning
4. Model Training
5. Model Performance Validation 

#### Feature Engineering

Our approach for feature engineering is as follows:
1. Only use correlated attributes from the outcomes of exploratory data analysis: `GrLivArea`, `GarageArea`, `TotalBsmtSF`, `1stFlrSF`, `YearBuilt`, `YearRemodAdd`, `KitchenQual`, `OverallCond`, and `OverallQual`.
2. Instead of feeding attributes directly, we will create bins out of target variable, group and aggregate attributes on the bin values (a concept inspired by histogram), and expand aggregrated features with sale prices matching the bins. 

This way of feature engineering achieves the following:
1. Efficient model training process due to small number of variables (9 instead of 23), hopefully without sacrificing the performance (due to the strong correlation nature). 
2. A feature engineering process that would allow solving this ML problem both as a regression and classification problem. The `SalePrice` bins can also be treated as house price classes. 

##### Binning SalePrice

We followed a simple approach. We created histogram out of the `SalePrice`, picked the highest value from each histogram bin as the chosen `SalePrice` bin values, and mapped the `SalePrice` to these chosen values. There were 39 bins. 

##### Categorical features

There were three categorical attributes. We followed the approach below:
1. For each categorical variable, we created a dummy variable for each of its categories. There were three categorical variables with 10, 10, and 5 categories, which created 25 dummy variables. Some category values were missing. So, we ultimately had 23 dummy variables. 
2. Then for each 23 dummy variables, we grouped them based on `SalePrice` bin values and aggregated the group using `sum` and `mean` functions. These process ultimately gave us 46 features.  

##### Numerical features

There were four chosen numerical attributes. For each of them, we grouped them based on `SalePrice` bin values and aggregated the group using `count`, `mean`, `max`, `min`, and `sum` functions. These process ultimately gave us 20 features taking the total count of feature to 66.

##### Temporal features

There were two chosen temporal variables, each representing years. we picked the highest year in one of the column and used as the most recent year. Then we calculated number of years using each value in the temporal columns from the most recent date using that value. These gave us 2 features, taking the total count of features to 68.

##### Combining features

After combining features from all three threads, we got 69 variables, 68 from the features and 1 from the `SalePrice` bin. Then we joined that dataset with the `SalePrice` and `SalePrice` bins. That gave us 70 x 1460 matrix. 

##### Splitting the features for training and testing

We decided split the feature matrix for training and testing purposes. The training dataset has the 80% of the dataset. Although, during hyperparameter tuning, we sampled 50% of the training dataset again. We will use the remaining 20% for testing model on known but unseen data.

#### Hyperparameter tuning

We decided to use Random Search technique over an [XGBoost estimator](https://xgboost.readthedocs.io/en/stable/) for the following parameter configurations:

- `n_estimators`: 500, 550, 600, 650, ..., 2000 
- `learning_rate`: 0.05, 0.06, 0.07 
- `max_depth`: 3, 5, 7
- `min_child_weight`: 1, 1.5, 2

The parameter optimisation ran for about 1.12 minutes. It generated an r2 score of 0.99 and 0.97 on full training feature and testing feature respectively with its best parameter, which happens to be `n_estimators`: 550, `min_child_weight`: 1.5, `max_depth`: 5, and `learning_rate`: 0.06. 

The estimator with the best parameter had the following feature importance score for the top 10 contributing features:

![Feature Importance Optimisation](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/feature_importance_opt.png)

#### Model training

The model is trained using XGBoost using the best parameters of the hyper parameter tuning over full training feature matrix. It only took a few seconds and generated generated an r2 score of 1 and 0.99 on full training feature and testing feature matrix respectively. 

The estimator had the following feature importance score for the top 10 contributing features:

![Feature Importance Optimisation](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/feature_importance_ml.png)

#### Saving and using the model

We saved the model in the `joblib` serialized format. The serialized modeled is loaded in memory again and tested against `inherited_houses` dataset that generated good looking predictions. 

#### Evaluating if predicted and actual values have similar correlations to housing attributes

We wanted to validate that the size (numerical) attributes would yield higher `SalePrice` for its larger values and lower for its lower values. Similarly validation needs to happen on condition and age attributes. 

To validate the hypothesis, we compared the correlation of housing attributes to both actual and predicted `SalePrice` based on testing dataset. The following table demonstrated that the actual and predicted (on data unused in training) `SalePrice` are quite strongly correlated. 

![Correlation Predicted Actuals](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/correlation_predicted_actuals.png)

The following scatterplots demonstrates that the predicted (on data unused in trainin) `SalePrice` generally increase with the that of the house size, condition, and age. It shows correlation to the columns mentioned above similarly to the actual sale price.

![Correlation Predicted Actuals Scatter](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/correlation_predicted_actuals_scatter.png)

### How key results are demonstrated in ML dashboard application

The objectives of the ML dashboard are given as follows:

1. Provide overview of the overall business problem
2. Demonstrate how to leverage the model to predict house prices
3. Demonstrate how to understand the training data and its correlation to target variable
4. Demonstrate how to understand the performance of the model

The ML dashboard is developed as a multipage Streamlit app that realizes the above objectives. The live version of the dashboard can be found [here](https://binita-heritage-housing-cb7da8771259.herokuapp.com/). 

![Dashboard page](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/dashboard_page.png)

The first page presents the same information as covered in [Business Requirements](https://github.com/fokhrun/heritage_housing/blob/documentation/README.md#business-requirements). The others pages are covered in the following sections.

#### House price prediction dashboard page

This is the most interactive part of the application that provides prediction results to inherited houses, but also provide prediction to any combination of housing attributes. 

The page has two subsections:

1. Providing input for housing attributes that will be used for prediction
2. Showing results

The first subsection includes a set of radio buttons, sliders, and buttons that allows a user to create a prediction result. Usually for house condition attributes radio buttons are used and for both housing size and age attributes sliders are used. To submit a prediction call or resetting buttons are used. Note that all attributes are included. The model is trained using selected set of attributes, which are also used to generate the predictions. The following image shows the input section.  

![prediction input](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/prediction_input.png)

The second subsection includes a table where prediction results are shown. This section already shows the `SalePrice` of the four inherited houses. However, a fifth `SalePrice` is shown which is the predicted price of custom inputs provided in the earlier subsection. 

The predictions are generated by the ML model developed in this project, which is loaded in memory with the dashboard. The predictions are marked as blue to highlight more prominently. The results also show selected attributes connected to the `SalePrice` to provide context to the predictions. The following images shows the prediction section prior to the custom prediction job and after custom prediction job. 

![prediction results](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/prediction_result.png)

![prediction results with input](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/prediction_result_input_based.png)

#### Training data analysis page

This page contains two subsections separated by tabs:

1. Basic training data analysis
2. Correlation of housing attributes to `SalePrice` attribute

The first subsection allows observing a view of the training data as well as the distribution of the attributes, data description statistics, and missing values. While the data view is always visible, the other options are only visible through expander, which a user can choose to see or not. Since there are 24 attributes, it is not easy to visualize. Therefore we only show 6 attributes at a time, which is selected using a number input. Check the following screenshot to get a brief understanding of the subsection:

![training data overview](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/training_data_overview.png)

The second subsection shows correlation of housing attributes to `SalePrice` variable. 
- It always show the information of highly correlated features
- It show scatterplot of highly correlated and low correlated features to `SalePrice` through expandders that a user can use to see or not. 

Check the following screenshot to get a brief understanding of the subsection:

![correlation to sale price](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/correlation_to_saleprice_page.png)

#### Training performance analysis page

This page contains three subsections separated by tabs:

1. Project hypothesis
2. Performance of ml model training that uses the best parameters from the hyper parameter tuning
3. Performance of hyperparameter tuning that tries to search the best parameters

The first subsection shows the same information as in - [Hypothesis](https://github.com/fokhrun/heritage_housing/blob/documentation/README.md#dataset).

The second subsection shows the performance of the ML model training. First of all, it shows what r2 and mse score the model has on training features (seen data) and testing features (unseen data). It also shows what are key parameters leveraged by the estimator used for prediction. In addition, it also shows how the estimator learned through an expander, which a user can choose to see. Finally, it shows how hypothesis in the projects are validated by the estimator through a set of scatterplot as well as key conclusiions. The following image shows the screenshot of the subsection:

![Training performance](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/modelling_performance_page.png)

The third subsection shows the hyperparameter tuning performance. First it shows the performance of the best estimator searched by the tuning process on both training and testing data on r2 and mse score. It also shows the performance of the top tried estimators, highlighting the best estimator. Both of these information are on expander, which allows user to choose to see or not.

![Training performance](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/hyperparameter_tuning_page.png)

To learn more about the training and hyperparameter tuning process and performance, check the section [How the trained model generates good predictions](https://github.com/fokhrun/heritage_housing/blob/documentation/README.md#how-the-trained-model-generates-good-predictions)

## Software Development

### Tech Stack

The main tech stack for this project is as follows:

- Programming Language: `Python` (3.11.6)
- Machine Learning and data analysis
    - `jupyter`: for interacting and visual heavy development
    - `pandas` (2.1.3) & `numpy` (1.26.2): large dataset wrangling
    - `xgboost` (2.0.3) & `scikit-learn` (1.3.2): ML and statistical work
    - `matplotlib` (3.8.2) & `seaborn` (0.13.0): data visualization
    - `kaggle` (1.5.16): for data sourcing
- Dashboard/Application: `streamlit` (1.29.0)
- Environment management: 
    - `pip` (23.3.1): Python library management
    - `venv` 
    - `python-dotenv` (1.0.0): Handle environment variables
- Version control: `GitHub`
- Project Managgement: `GitHub projects` 
- Production Environment: `Heroku`
- IDE: `VS Code` (1.85.1)

### Development Environment

The initial GitHub repo starts from the starter repo [milestone-project-heritage-housing-issues](Code-Institute-Solutions/milestone-project-heritage-housing-issues) provided by Code Institute. The starter repo includes a notebook folder structure, configuration examples of deploying streamlit dashboard to heroku, and a starter `requirements.txt` file. 

The project is cloned in a local VS code workspace. It leverages pip and venv module to install and isolate the execution environment. Notice that Python and other python library versions are more modern in this project then what is provided by starter repo. This came through repeated trial/error of what library version would work that would not come in the way of deploying streamlit dashboard to heroku. 

To start developing with this repo, use the following procedure:
```
1. Ensure that you have a Python version of at least Python v3.11 and pip v23.3.1
2. Clone the repository https://github.com/fokhrun/heritage_housing.git as heritage_housing in your local directory
3. Work inside heritage_housing
```

We strongly recommend using venv as follows:
```
1. python3 -m venv venv
2. .\venv\Scripts\activate
3. pip install -r .\requirements.txt
```

#### Working with notebooks for ML work
To start working with jupyter notebooks, run the command `jupyter notebook`. It will launch a jupter workspace. Start working with the existing notebooks. Currently there are three notebooks:
1. `data_collection.ipynb`: collecting data from kaggle.com
2. `exploratory_data_analysis.ipynb`: training data analysis including correlation study
3. `model_training.ipynb`: ML model training to be used for house price prediction

One can create a new one from the `Notebook_Template.ipynb`, the template came from the starter repo and is quite useful. 
However, we chose to handle environment variables differently than the template notebook recommends. Basically, not only, we define all secret information as environment variables, but we also use define all path and filename variables as environment variables. There are other ways to do it, but it is quite effective to use the same variable everywhere. 

#### Working with streamlit dashboard

To start working with streamlit, run the command `streamlit run Heritage_Housing.py`. Then it will start a streamlit application in the browser. 

The top page of the dashboard is always defined by the main app page, `Heritage_Housing.py`, in our case. This file contains project overview information. All the other pages are created under the pages folder. We have three such page python codes:

1. `1_Housing_Price_Prediction.py`: prediction generation
2. `2_Housing_Data.py`: Training data analysis overview
3. `3_Training_Performance.py`: Hypothesis and performance of the training process

Note that, the naming style influence how the dashboard page menu is created. For example, the number influences how the pages are ordered and the case and spacing of the files influences how the title of each page is shown in the menu. 

In case of streamlit dashboard, almost all data and file resources are cached. If you are changing the data or files, make sure to restart the service. 

### Testing

No automated unit tests were implemented for the Python files, due to lack of time. However, each Python file has been cleansed to remove pep8 issues. We did it using the warnings raised by the command

    `pylint pylint .\utils\ .\pages\ .\Heritage_Housing.py`

It generated a score of 10 out of 10. A few warning has been disabled to take care of unavoidable issues. 

### Testing notebook files

Only data shapes are tested using assert function, such as follows:

```assert inherited_houses_numerical.shape == (prediction_subset.shape[0], len(numerical_feature_names))```

The best way to run these tests are executing the notebooks top to bottom.

### Testing streamlit app

A few data shapes are tested using the assert function similar to the above. 
The dashboard has been tested manually as follows:

- `1_Housing_Price_Prediction.py` is tested by choosing the same inputs as the one of the inherited houses. If it generates the same `SalePrice` it is generating a consistant result. It is also tested, if clicking `Reset` button actually resets the prediction results to four inherited houses, once a prediction has been generated. 
- `2_Housing_Data.py` and `3_Training_Performance.py` are tested by clicking each tab and expander and observing if the information seems visually correct. They are further verified by comparing them against similar outputs in `exploratory_data_analysis.ipynb` and `model_training.ipynb` respectively.

### Deployment to production

The app is deployed in Heroku.

To prepare for the deployment, we used three files: 

- `runtime.txt`: should have `python-3.11.6`
- `setup.sh`: keep it as it came with the starter template. Add the line `mv .env_template_unix .env` so that correct env variables are loaded in the heroku machine. 
- `Procfile`: should have web: `sh setup.sh && streamlit run Heritage_Housing.py`

To deploy use the following instructions:

1. Create a new app in [Heroku](https://dashboard.heroku.com/apps)
2. Fill the form with the app name of your choosing, e.g., "heritage_housing" and "Europe" as the region
3. Stay with Heroku-22 stack
4. Select Deploy
    1. Choose "GitHub
    2. Search the GitHub repo name `heritage_housing`. Once the repo is found, connect the repo.
    3. Choose the default branch to deploy, i.e., the `main` branch. If you are working with another branch, choose accordingly.
    4. Click "Enable Automatic Deploys" which allows the app to be redeployed for every commit.
    5. Click "Deploy Branch" for the first manual push.
5. Select Open app to verify that the app has deployed.

### Fixed bugs

1. While choosing highly correlated variables, only moderate correlations were used due to an input mistake. It led to lower r2 score, as variables with high correlation were ignored. It was fixed afterwards. 
2. While running hyperparameter tuning, mse with positive scoring was chosen, where it should have been a negative score. That also led to a lower r2 score. It was fixed afterwards.

### Unfixed bugs

Some element of the dashboards loads a bit slow the first time. This is because some of the plots are calculated on-demand. These plots should be pre-computed and loaded as a file in the dashboard.

## Planning & Execution

The project followed a simple agile method, with the 5 main epics:
- Bootstrap
- Data collection
- Data analysis
- Model training
- Dashboard
- Documentation

The epics were executed with one or more user stories in the GitHub Project [Heritage Housing](https://github.com/users/fokhrun/projects/3/views/1?filterQuery=). The following image provides a snapshot of the project:

![Tracking user stories in GitHub Project](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/github_project_heritage_housing.png)

The project is designed as a kanban board with three columns: `To Do`, `In Progress`, `Done`.

Each user story is created by adding an item in the `To Do` column. When work starts for a user story, it is moved to `In Progress`. When the work regarding the user story finishes, it is moved to `Done` column. The following figure demonstrates a user story. 

![User story](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/user_story.png)

### Execution

Each user story is linked to a GitHub issue as demonstrated by the following figure. It allows connected the user story to relevant GitHub branc/pull request. 

![Issue connected to user story](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/issues_connected_to_user_stories.png)

The execution of the user story typically starts at the codebase, with a branch off the main branch. Once the development is done, the branch is merged to the main branch using a pull request (see below figure).

![Pull request](https://github.com/fokhrun/heritage_housing/blob/documentation/doc_images/pull_request.png)

### User Stories

|  Epic | User Stories |
|---|---|
| Bootstrap | [Starter repository](https://github.com/users/fokhrun/projects/3/views/1?filterQuery=&pane=issue&itemId=46559138) |
|  | [Local development environment](https://github.com/users/fokhrun/projects/3/views/1?filterQuery=epic%3ABootstrap&pane=issue&itemId=46693177) |
| Data collection | [Prepare for data collection](https://github.com/users/fokhrun/projects/3/views/1?filterQuery=epic%3A%22Data+collection%22&pane=issue&itemId=46690864)
|  | [Data collection notebook](https://github.com/users/fokhrun/projects/3/views/1?filterQuery=epic%3A%22Data+collection%22&pane=issue&itemId=46560003) |
| Data Analysis | [Exploratory data analysis](https://github.com/users/fokhrun/projects/3/views/1?filterQuery=epic%3A%22Data+Analysis%22&pane=issue&itemId=46694465)
| Model Training | [Model training, optimization and validation](https://github.com/users/fokhrun/projects/3/views/1?filterQuery=epic%3A%22Model+Training%22&pane=issue&itemId=46690073) |
| Dashboard | [Dashboard planning, designing, and development](https://github.com/users/fokhrun/projects/3/views/1?filterQuery=&pane=issue&itemId=46690128) |
|  | [Dashboard deployment and release](https://github.com/fokhrun/heritage_housing/issues/12) |
| Documentation | [Documentation](https://github.com/users/fokhrun/projects/3/views/1?filterQuery=&pane=issue&itemId=49358304) |

## Future Improvements

1. Bulk of the code used in the jupyter notebooks are written as flat Python script. A sizable part of the code should be placed inside functions in order to reduce similar looking codes. It would also allow testing part of jupyter notebook code in a better way.
2. Implement automated unit testing for python functions. 
3. More variables can be used in feature engineering to see if it effects modeling performance without consuming too much time. 
4. Feature engineering with plain variables should be tried to understand if it effects modeling performance.
5. Other hyper parameter tuning approach, such as Grid Search, HyperOpt, etc., should be tried to understand if it effects modeling performance and time. 
6. Other estimators, such as Random Forest, LightGBM, etc., should be tried to compare modeling performance.
7. A classification based technique can tried to compare modeling performance. 

## Credits 

* In this section, you need to reference where you got your content, media and extra help from. It is common practice to use code from other repositories and tutorials, however, it is important to be very specific about these sources to avoid plagiarism. 
* You can break the credits section up into Content and Media, depending on what you have included in your project. 

### Content 

- The text for the Home page was taken from Wikipedia Article A
- Instructions on how to implement form validation on the Sign-Up page was taken from [Specific YouTube Tutorial](https://www.youtube.com/)
- The icons in the footer were taken from [Font Awesome](https://fontawesome.com/)

### Media

- The photos used on the home and sign-up page are from This Open Source site
- The images used for the gallery page were taken from this other open-source site



## Acknowledgements (optional)
* In case you would like to thank the people that provided support through this project.

